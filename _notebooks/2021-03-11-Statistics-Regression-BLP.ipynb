{
 "cells": [
  {
   "source": [
    "# Best Linear Predictor (BLP)\n",
    "> Notes.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [stats]\n",
    "- image: images/logo.JPG\n",
    "- layout: post\n",
    "- author: \"<a href='https://pradeeptadas.github.io/'>Pradeepta Das</a>\"\n",
    "- permalink: /ghtop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "So CEF can be non-linear. Here we will explore what we do to get a linear predictor? \n",
    "\n",
    "\n",
    "### Linear CEF\n",
    "$m(x) = E(y|\\textbf{x})$ is linear in x. \n",
    "\n",
    "$$ m(\\textbf{x}) = \\textbf{x'}\\beta + e \\implies y = \\textbf{x'}\\beta + e  $$\n",
    "Here the error doesnt have the mean error property! Because we are breaking down as linear vs nonlinear part. \n",
    "\n",
    "Generic notation of a linear model: \n",
    "\n",
    "$$ m(x) = \\textbf{x'}\\beta, x = (x_1, x_2, ... x_k)'$$\n",
    "\n",
    "But typically x includes a constant. \n",
    "\n",
    "So it is convenient to write as $y = \\alpha + \\textbf{x'}\\beta + e$\n",
    "But if you take expectations on both sides ad solve for $\\alpha$ and substitute: \n",
    "\n",
    "$y - \\mu_y = (x - \\mu_x)' \\beta + e \\implies \\overline{y} = \\overline{x'} \\beta+ e$\n",
    "\n",
    "So, you can de-mean any series and write in this format! \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the conditional mean is the best predictor of y. \n",
    "But what is the best predictor $m(x)$ in the linear family of regrressors?\n",
    "\n",
    "Assumptions are, $Q_{xx} = E(xx')$ is positive definite. This is going to be a second moment matrix. \n",
    "\n",
    "\n",
    " \n",
    "**Best Linear Predictor**\n",
    "\n",
    "- MSE is $S(\\beta) = E(y - \\textbf{x}'\\beta)^2$\n",
    "\n",
    "- The best linear predictor of y given $\\textbf{x}$ is $P(y|x) = \\textbf{x}'\\beta$\n",
    "\n",
    "- BLP coefficient is given by $\\beta = argmin_{b \\in \\mathbb{R}^k} S(b)$. The BLP coefficients are also called the Linear Projection Coefficient. \n",
    "\n",
    "- $\\hat{\\beta}$ is unique and given by $(E(xx')^{-1} E(xy))$ \n",
    "\n",
    "- The BLP is given by $P(y|x) = \\textbf{x}'\\beta = \\textbf{x}' (E(xx')^{-1} E(xy)) $\n",
    "\n",
    "- The error $e = y - \\textbf{x}'\\beta$ exists and satisfies $\\boxed{E(\\textbf{x}e) = 0}$\n",
    "\n",
    "This means for each i, $E[X_i e] = 0$ i.e. $X_i$ and e are orthogonal. \n",
    "\n",
    "\n",
    "- If x is a constant then $E(e) = 0$, however for CEF, it was always true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For BLP, it must be true that the errors are uncorrelated with $x_is.$ \n",
    "\n",
    "$\\beta$ is BLP $ <=> E(xe) =0$\n",
    "\n",
    "- This is the moment condition (in the moment estimator!). So the BLP can be written as a moments estimator!! \n",
    "\n",
    "- So, all the properties of moments estimators are true in this case!\n",
    "\n",
    "- The moment that finds BLP is that errors are othoginal to the $x_i$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Prediction with Constant Term**\n",
    "\n",
    "If y has a constant term i.e. $y = \\alpha + \\textbf{x'} \\beta + e$ \n",
    "\n",
    "$\\implies \\alpha = E(y) - E(\\textbf{x'} \\beta) = \\mu_y - \\mu'_x\\beta $\n",
    "\n",
    "$\\implies \\beta = Var(x)^{-1} Cov(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Predictor Error Variance**\n",
    "$\\sigma^2 = E(y - \\textbf{x'}\\beta)^2 = Q_{yy} - Q_{yx} Q^{-1}_{xx} Q_{xy}$\n",
    "\n",
    "This is the variance of the errors from the BLP of y on x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Normality\n",
    "\n",
    "- (y, x) are jointly normal\n",
    "- This means (e, x) are jointly normal\n",
    "- $E(e) = 0$ and $E(xe) = 0$ $\\implies Cov(e, x) = 0$.\n",
    "- e and x are jointly normal and uncorrelated and this independent! This is only for normality! \n",
    "- For Independence, $E(e|x) = E(e) = 0$\n",
    "- So, This is CEF!\n",
    "- Under Joint Normal, the linear projection is a CEF!\n",
    "- So, therefore BLP is the best predictor among all (including nonlinear) predictors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Let $z_1, z_2$ are two independent N(0,1) RVs and\n",
    "\n",
    "$x = \\mu_x + \\sigma_x z_1$\n",
    "\n",
    "$y = \\mu_y + \\sigma_y (\\rho z_1 + \\sqrt{1 - \\rho^2} z_2)$\n",
    "\n",
    "For this $\\sigma^2_x = Var(x)$, $\\sigma^2_y = Var(y)$, $Cov(x,y) = \\rho$\n",
    "\n",
    "$\\beta = \\rho \\frac{\\sigma_y}{\\sigma_x}$\n",
    "\n",
    "$Var(e) = (1 - \\rho^2)\\sigma_y^2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}