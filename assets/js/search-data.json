{
  
    
        "post0": {
            "title": "Notes - Sequence Model - Deep Learning Specialisation",
            "content": "RNN . RNNs share the parameters for each layer. Each layer can output the y vectors. | The backpropagation here is called backpropagation through time because we pass activation a from one sequence element to another like backwards in time. | . Language model and sequence generation . RNNs do very well in language model problems. | To train the language model, we basically have to learn the conditional probabilities between layers. Thats what you have to train the model for. The loss function used here is cross-entropy loss. Do a summation of y log y for all elements in the corpus and do this for all timesteps. | Character level language model also exists. The lab with Dinasour names. The model we used had the following layers in Optimization loop (there is initialization of parameters at the very beginning). Forward propagation to compute the loss function | Backward propagation to ocmpute the gradient with respect to the loss function | Clip the gradients, update your parameters with the gradient descent udpate rule. | . | At each time step RNN tries to predict what is the next character given the preivious characters. | . GRU LSTM . These are used to have a memory. LSTM is more generic version and was invented first. GRU was discovered recently but has been gaining popularity gradually and slowly. Its easier to scale tthe GRUs for different problems. But historically LSTM has been the most proven choice. | . Bidirectional RNN . has both forward and backward direction. This uses the future parts as well to make the prediction. The blocks used here can also be LSTM! Bidirectional LSTM is common in NLP problems. | . Deep RNNs . | .",
            "url": "https://pradeeptadas.github.io/bquant/deep-learning/2021/03/15/rnn.html",
            "relUrl": "/deep-learning/2021/03/15/rnn.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Projections . To fully understand the linear model we will develop a geometric interpretation. . When &lt;x, z&gt; = 0, then $cos( theta) = 0$ and $x$, $z$ are orthogonal. . | Any vector $y in mathbb{R}^n$ can be written as $y = hat{y} + y^{ perp}$, where $ hat{y} in S, y^{ perp} in S^{ perp}$ . | $ hat{y}$ is the projection of $y$ on $S$ . | Matrices P and M such that $Py = hat{y}, My = y^{ perp}$ . | . How to compute P and M? . $$P = X(X&#39;X)^{-1}X&#39;$$ $$M = I - P = I - X(X&#39;X)^{-1}X&#39;$$ . $PX = X $ . $MX = 0$ . P and M are symmetric and idempodent. P and M are orthogonal $PM = 0$ . Lets connect this to regression now. . $y = hat{y} + y^{ perp}$ $ implies hat{y}&#39; + y^{ perp} = 0$ $ implies E( hat{y}y^{ perp}) = 0$ . Connection to BLP . Recall that for BLP: $y = x&#39; beta + e$ and $E(xe) = 0$ . | BLP is equivalent to a projection of the dependent variable $y$ on the linear space spanned by the independent variables $x$! The error term is perpendicular to this. . | . For OLS: . $ beta = (X&#39; X)^{-1}X&#39;y$ . $ hat{y} = Py$ . $ hat{e} = My$ . Least Squares as a Linear Projection! .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/13/Statistics-Regression-Projections.html",
            "relUrl": "/2021/03/13/Statistics-Regression-Projections.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Properties of OLS",
            "content": "There is no &quot;the&quot; OLS regression. . Starting with very strong assumption: . i.i.d. random sample (this doesn&#39;t hold true in timeseries context in fin) | . We have data. Data comes from realisations of (y, x). Data is generated by the random variables. We assume that the sample is iid for now -&gt; This implicate that the ordering of data doesnt matter. . We want to estimate $ beta$. There are many equivalent ways to derive the optimal estimator $ hat{ beta}$ for the linear model. . Minimizing the sum of squared errors. (hence least square estimator) | Apply method of moments to estimate BLP $ beta = (E(xx&#39;))^{-1}E(xy)$ | Apply MM to BLP condition $E(xe) = 0$ | Method1: Minimizing the sum of squared errors . We derived the BLP as $ y_i = x&#39;_i beta + e_i $ . Where $ beta = argmin_{ beta in mathbb{R}^k} S( beta)$ is the minimizer of the expected squared error $$ S( beta) = E(y_i - x&#39;_i beta)^2 $$ . and has the solution $ beta = (E(x_i x&#39;_i))^{-1} E(x_i y_i)$ . Now for the complete dataset: iid ${(y_1, x_1), ... (y_n, x_n)}$ . For each datapoint we can plugin $y_i - x_i beta$ . SSE of sample is $$ S_n( beta) = frac{1}{n}SSE_n( beta) $$ . $ hat{ beta}$ is called the least squares (LS) estimator. . $y_i$ is scalar and $x_i$ is a $k times 1$ vector. . $$ hat{ beta} = ( sum_{i=1}^{n} x_i x&#39;_i)^{-1} sum_{i=1}^{n} x_i y_i$$ . Method2: Moment Estimation of BLP . Equivalently, least square can be written as moment estimator. . $ beta = Q^{-1}_{xx}Q_{xy} = (E(xx&#39;))^{-1}E(xy)$ . $ hat{ beta} = hat{Q}^{-1}_{xx} hat{Q}_{xy}$ . Moment estimators for $Q_{xx}$ and $Q_{xy}$: . $$ hat{Q}_{xy} = frac{1}{n} sum_{i=1}^{n} x_i y_i$$ $$ hat{Q}_{xx} = frac{1}{n} sum_{i=1}^{n} x_i x&#39;_i$$ . This is exactly same as Method1. . Method3: BLP Condition E(xe = 0) . Define the fitted values as residuals . $$ hat{y_i} = x&#39;_i hat{ beta} $$ $$ hat{e_i} = y_i - x&#39;_i hat{ beta}$$ . The sample equivalent of $E(xe) = 0$ is: . $$ 0 = frac{1}{n} sum_{j=1}^{n} x_i hat{e}_i = frac{1}{n} sum_{j=1}^{n} x_i ( hat{y}_i - x&#39;_i hat{ beta}) $$ . $$ hat{ beta} = ( sum_{i=1}^{n} x_i x&#39;_i)^{-1} sum_{i=1}^{n} x_i y_i$$ . Here moment condition to estimate the condition. in method 2 we used moment generator to $ beta$. . For intercept only model: . $y_i = mu + e_i$ . $E(e_i) = 0$ . So, the MM and MLE estimator of population mean is the sample mean. . And the OLS regressor is also the sample mean! Its a constant. . Sample mean = $ hat{ mu}$ = $ frac{1}{n} sum_{i=1}^{n}y_i$ . Matrix Form . $$ sum_{i=1}^{n} x_i x&#39;_i = X&#39;X$$ $$ sum_{i=1}^{n} x_i y_i = X&#39;y$$ . Least Square Estimator is $$ hat{ beta} = (X&#39;X)^{-1}(X&#39;y)$$ . The estimated version of the model is $y = X hat{ beta} + hat{e}$ and the residual vector is $ hat{e} = y - X hat{ beta}$ . $$ X&#39; hat{e} = 0$$ .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/12/Statistics-Regression-OLS.html",
            "relUrl": "/2021/03/12/Statistics-Regression-OLS.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "CEF - Conditional Expectation Function",
            "content": "Theorem Law of Iterated Expectations . If $E(|y|) &lt; infty$, then for any random variable $ textbf{x}$: $$E(E(y| textbf{x})) = E(y)$$ . General Law of Iterated Expectations . If $E(|y|) &lt; infty$, then for any random variable $ textbf{x}_1$, $ textbf{x}_2$: $$ E(E(y| textbf{x}_1, textbf{x}_2)| textbf{x}_1) = E(y| textbf{x}_1)$$ . Conditioning Theorem . If $E(|g( textbf{x})y|) &lt; infty$, . $$ E(g( textbf{x})y| textbf{x}) = g( textbf{x}) E(y| textbf{x})$$ . and . $$ E(g( textbf{x})y) = E(g( textbf{x}) E(y| textbf{x}))$$ . CEF Error . The CEF error e is the differnce between y and CEF $m( textbf{x})$: $e = y - m(x)$ . $E(e| textbf{x}) = E(y-m( textbf{x}) | textbf{x}) = E(y| textbf{x}) - m( textbf{x}) = 0$ . Law of Iterated expectations shows more!: $E(e) = E(E(e| textbf{x})) = E(0) = 0$ . For any $h( textbf{x})$ : $E(h( textbf{x})e) = 0$ . Any predictor $g(x)$ is the CEF if and only if: $ E(e_g| textbf{x}) = 0$ . Example: Intercept Model: Here $m(x)$ is a constant $ = E(y) = mu$ . Variance of the CEF Error . If we didnt observe x, m(x) would be a constant which is mean of y i.e. $E(y)$. But given that we observe x, it gives a lot more information to us. $m(x)$ is a function of x and we can predict how y would behave with different x. . How can we measure how much extra information x is giving? Ans: by computing variance of error. If there is less variance, then more information in x. High variance means less information in x. The variance measures the variation in y that is not explained by the conditional mean $E(y|x)$. . $Var(e) = E((e- E(e))^2) = E(e^2)$. The error variance depends on x. Because, lets say 2 models: . $y = E(y|x_1) + e_1$ . $y = E(y|x_1, x_2) + e_2$ . $ implies sigma_1^2 neq sigma_2^2$ . Theorem: $Var(y) geq Var(y - E(y|x_1))$ (more info $ implies$ less variance) . Example . Suppose z = (x, y) are jointly normal with zero means $ mu = (0, 0)&#39;$ and covariance matrix | . $$ Sigma = begin{pmatrix} 1 &amp; rho rho &amp; 1 end{pmatrix} $$The CEF of y given x is $E(y|x) = m(x) = rho x $ . The variance of CEF error is $Var(e) = 1 - rho^2$ . Best Predictor . Best predictor $g(x)$ is one that minimize the MSE = $E(y - g(x))^2$ . The CEF $m(x)$, regardless of the joint distribution of $(y, textbf{x})$, minimizes the MSE! . The conditional variance of y given x is $$ sigma^2(y| textbf{x}) = sigma^2( textbf{x}) = Var(y| textbf{x}) = E((y - E(y|x))^2|x)= E(e^2| textbf{x})$$ . For the above example, if the correlation is 1, the conditional variance of (y| textbf{x}) is 0! . Conditional variance is how much info is left in y after we remove x. . The unconditional variance of the error is teh average of conditional variance. $ sigma^2 = E(e^2) = E(E(e^2| textbf{x})) = E( sigma^2( textbf{x}))$ . Any multivariate rv $z = (y, x)$ can be decomposed as: $$ y = m(x) + sigma(x) epsilon $$ . Where, $m(x) = E(y|x), epsilon = frac{e}{ sigma(x)}, E( epsilon|x) = 0, Var( epsilon|x) = 0$ . Often conditional variance is ignored. . $ sigma(x) = sigma =$ const then it is homoskedasticity. If teh volatility of error is not constant then heteroskedastcity. . CEF Derivative . How does CEF vary with small change in x? The marginal effect of $x_1$ is . $$ Delta_1 m( textbf{x}) = frac{ partial}{ partial x_1} m(x_1, ..., x_k)$$ . note that this derivative doesnt measure change in y, but change in the conditional expectation of y. . Summary: $m(x)$ minimizes MSE. $E(e|x) = 0; E(e) = 0; y = m(x) + sigma(x) epsilon$ . CEF can be non-linear. So, next is to understand, how to get a linear regressor? . Similarly, CEF: $E(e| textbf{x}) = 0 &lt;=&gt; E(e|x_j)$ . $E(e|x)$ implies $E(xe) = 0$ . So CEF is more powerful than BLP. And both are examples of moment estimators. .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/11/Statistics-Regression-CEF.html",
            "relUrl": "/2021/03/11/Statistics-Regression-CEF.html",
            "date": " • Mar 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Example",
            "content": "So CEF can be non-linear. Here we will explore what we do to get a linear predictor? . Linear CEF . $m(x) = E(y| textbf{x})$ is linear in x. . $$ m( textbf{x}) = textbf{x&#39;} beta + e implies y = textbf{x&#39;} beta + e $$ Here the error doesnt have the mean error property! Because we are breaking down as linear vs nonlinear part. . Generic notation of a linear model: . $$ m(x) = textbf{x&#39;} beta, x = (x_1, x_2, ... x_k)&#39;$$ . But typically x includes a constant. . So it is convenient to write as $y = alpha + textbf{x&#39;} beta + e$ But if you take expectations on both sides ad solve for $ alpha$ and substitute: . $y - mu_y = (x - mu_x)&#39; beta + e implies overline{y} = overline{x&#39;} beta+ e$ . So, you can de-mean any series and write in this format! . We know that the conditional mean is the best predictor of y. But what is the best predictor $m(x)$ in the linear family of regrressors? . Assumptions are, $Q_{xx} = E(xx&#39;)$ is positive definite. This is going to be a second moment matrix. . Best Linear Predictor . MSE is $S( beta) = E(y - textbf{x}&#39; beta)^2$ . | The best linear predictor of y given $ textbf{x}$ is $P(y|x) = textbf{x}&#39; beta$ . | BLP coefficient is given by $ beta = argmin_{b in mathbb{R}^k} S(b)$. The BLP coefficients are also called the Linear Projection Coefficient. . | $ hat{ beta}$ is unique and given by $(E(xx&#39;)^{-1} E(xy))$ . | The BLP is given by $P(y|x) = textbf{x}&#39; beta = textbf{x}&#39; (E(xx&#39;)^{-1} E(xy)) $ . | The error $e = y - textbf{x}&#39; beta$ exists and satisfies $ boxed{E( textbf{x}e) = 0}$ . | . This means for each i, $E[X_i e] = 0$ i.e. $X_i$ and e are orthogonal. . If x is a constant then $E(e) = 0$, however for CEF, it was always true! | . For BLP, it must be true that the errors are uncorrelated with $x_is.$ | . $ beta$ is BLP $ &lt;=&gt; E(xe) =0$ . This is the moment condition (in the moment estimator!). So the BLP can be written as a moments estimator!! . | So, all the properties of moments estimators are true in this case! . | The moment that finds BLP is that errors are othoginal to the $x_i$. . | . Linear Prediction with Constant Term . If y has a constant term i.e. $y = alpha + textbf{x&#39;} beta + e$ . $ implies alpha = E(y) - E( textbf{x&#39;} beta) = mu_y - mu&#39;_x beta $ . $ implies beta = Var(x)^{-1} Cov(x,y)$ . Linear Predictor Error Variance $ sigma^2 = E(y - textbf{x&#39;} beta)^2 = Q_{yy} - Q_{yx} Q^{-1}_{xx} Q_{xy}$ . This is the variance of the errors from the BLP of y on x. . Joint Normality . (y, x) are jointly normal | This means (e, x) are jointly normal | $E(e) = 0$ and $E(xe) = 0$ $ implies Cov(e, x) = 0$. | e and x are jointly normal and uncorrelated and this independent! This is only for normality! | For Independence, $E(e|x) = E(e) = 0$ | So, This is CEF! | Under Joint Normal, the linear projection is a CEF! | So, therefore BLP is the best predictor among all (including nonlinear) predictors! | .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/10/Statistics-Regression-BLP.html",
            "relUrl": "/2021/03/10/Statistics-Regression-BLP.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Partial Differential Equation",
            "content": "",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/05/PDE.html",
            "relUrl": "/2021/03/05/PDE.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Title",
            "content": "Difference Equation . It defines how an elements in a sequence are related. . Consider $y : mathbb{N} rightarrow mathbb{R}$ defined by the recursive relations. . $$y_1 = c$$ $$y_{n+1} = ay_n$$ . $$ implies boxed{y_n = a^{n-1}c}$$ . This is 1st order homogeneous linear difference equation. . Similarly for kth order: . Law of motion: $$y_{n+k} + a_{k-1}y_{n+k-1} + ... a_0 y_n = 0 $$ | . Initial Conditions for the law of motions: (k initials). $y_0 = c_0, ... y_{k-1} = c_{k-1}$ to compute the initial $y_{n+k}$ . | a&#39;s are law of motions. c&#39;s are initial conditions. . | . 2nd Order . We have the characteristic equation $r^2 + a_1 r + a_0 = 0$ . Roots can be . real and distict $ implies y_n = A(r_+)^n + B(r_-)^n$ | real and same $ implies y_n = A(r)^n + Bn(r)^n$ | complex $ implies y_n = A(r)^n cos(n theta) + Bn(r)^n sin(n theta)$ | . Ordinary Differential Equation (ODEs) . $$ y&#39; = f(y, x) $$ $$ y(0) = c $$ . Wehn differential eqn is solution? | Is it unique? | When depends continuously on parameters? - this is important from numerical perspective!- because $y_p$ might be very different from $y_{p+ epsilon}$ | . Lets compare with linear equation. $y = Ax$ . A invertible $ implies$ unique solution | not invertible $ implies$ no / infinite solutions | . This carries over to linear ODEs. . Picard&#39;s Theorem . Consider the first order ODE: $ frac{dy}{dt} = f(t, y(t))$, $y(t_0) = y_0$. Givem that f is Lipschitz continuous (i.e. Holder with exponent 1) in y and continuous in t. Then there exists a unique solution to the ODE in a neighbourhood of $t+0$. .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/04/ODEs.html",
            "relUrl": "/2021/03/04/ODEs.html",
            "date": " • Mar 4, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "LDA (Linear Discriminant Analysis)",
            "content": "M data obervations, each in $ mathbb{R}^N$. i.e. $x in mathbb{R}^N, m = 1, ..., M$ . We know that observations come from two types of points. A-points and B-points. . Both points are normally distributed and different variance-covariance matrices. . For the points, we have to classify each of them belong to type-A or type-B i.e. $t^{&#39;}_m in {A, B}$ . minimize the likelihood: . min $ alpha^A mathbb{P}(t^{&#39;} = A | t=B) + alpha^B mathbb{P}(t^{&#39;} =B | t=A)$ . So we want to find two subplanes separated by a hyperplane. One side is type A and one is type B. i.e. find $v in mathbb{R}^N$ and $c in mathbb{R}$ such that . $v^T x^m &lt; c implies t^{&#39;} = A$ | $v^T x^m &gt; c implies t^{&#39;} = B$ | . For a 1-D case, probability of misclassifying A is $1 - Phi( frac{c- mu_A}{ sigma_A})$. . Similarly, for misclassifying B is $ Phi( frac{c- mu_B}{ sigma_B})$. . We want to minimize this probability. . Proof . Without loss of generality $v = 1$ . $c = frac{ sigma^2_B}{ sigma^2_A + sigma^2_B} mu_A + frac{ sigma^2_A}{ sigma^2_A + sigma^2_B} mu_B$ . For multivariate case, . $v = ( Sigma_A+ Sigma_B)^{-1} $ . For linear discriminant analysis we generally assume $ Sigma_A = Sigma_B$. For this case $ alpha_A = alpha_B = 1$, so same weight on A and B misclassification. . We generally go for Quadratic Discriminant Analysis (QDA) when $ Sigma_A neq Sigma_B$. . Nonlinear Classification Problem . For the non-linear data, LDA is not a suitable method as we cant find a line to separate these. . Here, Machine Learning comes into play! In linear case, we try to increase the dimensionality for more complex problems. But in Machine learning it does the simple thing over and over again. . Lets consider a nonlinear approximation $$ (Ax + b)_+.$$ Here the nonlinear operation is $(x)_+$. It&#39;s a ReLU operator. We do this elementwise on the vector and then we sum the vectors by multiplying with $1$ vector. $F = textbf{1&#39;F}$ These are not so nice because they are piece-wise linear but not continuously differentiable. . For different A and B, we will get very different level sets. So, by iterating again and again we can generate a continuous function! Piece-wise linearity and continuity kept, regardless of N. Is it continuously diffentiable? . Basically this is what the neural network optimization does! .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/03/Math-LDA-and-Non-Linear-Classification.html",
            "relUrl": "/2021/03/03/Math-LDA-and-Non-Linear-Classification.html",
            "date": " • Mar 3, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Integration",
            "content": "Integration is area under graph. . Reimann / Darboux Integral . Assume a bounded function $f: [a,b] rightarrow mathbb{R}$ Here we partition the interval into smaller ones. For that we choose finite sequence of points $a = x_0 leq x_1 leq x_2 .. x_n = b$ . How given the partition define, . $$M_i = {sup}_{x_{i-1} leq x leq x_i f(x)}$$ $$m_i = {inf}_{x_{i-1} leq x leq x_i f(x)}$$ $$ Delta x_i = x_i - x_{i+1}$$ $$U(P, f) = sum_i M_i Delta x_i$$ $$L(P, f) = sum_i m_i Delta x_i$$ . Now we define an uppper integral $ int_{a}^{b} inf_{P} U(P, f)$ - which is infimum over all partitions of the sums that we get for a specific partition. (i.e. make finer partitions as well, and sum. Over all such parititon sums, take the minumum one). Similarly lower integral - $ int_{a}^{b} sup_{P} L(P, f)$ . If f is bounded from below and above in [a,b], then both the upper and lower integral exists. They converge but might converge to different numbers! . But $ int_{a}^{b} sup_{P} L(P, f) leq int_{a}^{b} inf_{P} U(P, f)$ So this says that the lower sum is less than equal to upper sum. This will exists in limit as well. . Reimann Integral says that, if both these integrals are the same then the Reimann integral exists. | . $$ int_{a}^{b} f(x) dx = int_{a}^{b} sup_{P} L(P, f) = int_{a}^{b} inf_{P} U(P, f)$$ . If f is bounded on [a,b] and continuous except for a finite number of points, then f is Riemann integrable, $f in R$ . | This intuition is captured by the above image. Becase as partition becomes very high, then both will converge to the same number. . | For the finite number of discontinuity - intuition is that because we are shiriking the partitions to very small, then it will converge to the same number. . | . Integration and Sequences . If we have a sequence of functions $f_n : [a, b] rightarrow mathbb{R}$ is said to converge uniformly to the function f if $f_n rightarrow d_c f$ i.e. if it converges in the supremum norm. . $d_c$ is the norm / distant function. $d_c (f_1, f_2) = || f_1 - f_2 ||_{dc} = sup_{x in [a, b]} |f_1 - f_2|$. . i.e. supremum norm is the maximum distance between $f_1$ and $f_2$ in the interval [a,b] . This is stronger than pointwise convergence. This is similar as we had discussed about uniform and pointwise continutity. . | Pointwise: we chose x and check if the functions are converging at that point to f. i.e. $ forall x ~~~ forall epsilon &gt; 0 ~~~ exists n: |f_n(x) - f(x)| &lt; epsilon $ Here we are allowed to chose n based on x. . | . Uniform convergence: Here we choose n and that should be valid for all x. $ forall epsilon &gt; 0 ~~~ exists n ~~~ forall x: |f_n(x) - f(x)| &lt; epsilon $ | . We want to check if we can switch the order of integration and limit . Theorem says that, . | If $f_n$ is a Reimann, for each n and $f_n$ converges uniformly to f, then . | . $$ lim_{n rightarrow infty} int_a^b f_n(x) dx = int_a^b lim_{n rightarrow infty} f_n(x) dx = int_a^b f(x) dx $$ . This doesn&#39;t hold for regular pointwise convergence. | . Why do we need uniform convergence? Any counter-example? . Example is . Doubt: How is f_n pointwise converging here? what is the f? . Fundamental Theorem of Calculus . If $f in R$ (Reimann Integrable) on [a,b] and if there is a differentiable function $F$ on [a, b], fuch that $F&#39; = f$, then . $$ int_a^b f(x) dx = F(b) - F(a)$$ . F is a primitive function (anti-derivative) of f. . This leads to Integration by parts: $ int_b^b F(x) g(x) dx = F(b)G(b) - F(a)G(a) - int_a^b f(x) G(x) dx$ . $ implies int_a^b frac{ partial}{ partial x} (F cdot G) dx = F cdot G |_a^b$ . Properties that hold almost everywhere . Lebesgue integral is more general than the Riemann integral. This is based on Measure theory. A lot of function&#39;s area is not defined by Reimann. So we define Lebesgue integral for those cases. But here that route is not taken. But we will use measure-zero property which is common in lebesgue integral. . almost everywhere: . Consider a set $A subset mathbb{R} $. $B = cup_{i=1}^{ infty} (a_i, b_i)~~~$ (Union of open intervals is open) - is an open cover of $A$ if $A subset B$ . What is the size of A? - its going to be less than equal to the total size of all such intervals. i.e. . If the intervals $(a_i, b_i)$ are disjoint, then their total lenth is $L(B) = sum_i (b_i - a_i)$ | If the intervals are not disjoint, then their total length is less than $L(B)$ | . The set $A$ is of measure zero if for any $ epsilon &gt; 0$, there is an open cover, $B_{ epsilon}$ of A, such that $L(B_{ epsilon}) leq epsilon$. . A function, $f : mathbb{R} rightarrow mathbb{R}$ has a property (e.g., being positive) almost everywhere if the property is not satisfied on a set measure zero. . Why it is useful? Properties that fail on zero measure, i.e. properties that hold almost everywhere, they are enough for whatever you want to hold. We don&#39;t have to require the function to be true everywhere, as long as they are true almost everywhere. . Classical example for which the property fails. -&gt; f(x) = 1 for $x in mathbb{Q}$, 0 otherwise $ mathbb{Q}$ is of the form p/q. . The set of rational number is dense in the set of real numbers, it will contain a lot of 1&#39;s and 0&#39;s. But this is zero almost everywhere. . We can show it by showing measure zero. Choose $B_i = (x_i - epsilon / 4 2^{-i}, x_i + epsilon / 4 2^{-i})$ . So it shows that the set Q has meaure zero. So, function f is zero almost everywhere because it fails to have the property in zero measure. . This is not reimann. So the lower is always 0 and upper is 1. Because there are rational and non-rational numbers in any interval. . Necessary and sufficient Condition to be Riemann . The bounded function f is Riemann integrable on [a,b] if and only if f is almost everywhere continuous on [a,b]. . Sufficient condition for a function to be reimann is f has to be bounded on [a, b] and has to be continuous except for at a finite number of points. . The finite discontinuous points would be in zero measure, so it would be covered in the first condition above in the necessary and sufficient condition. Also the infinite number of ups and downs has to be zero measure. In the example of $1$ if $x in mathbb{Q}$ is not really finitely discontinuous. So its not Reimann. . The failure of Reimann is not so much important why we need Ito. . Riemann-Stieltjes Integral . Its a generalisation of Riemann. . We assume bounded $f : [a, b] rightarrow mathbb{R}$, partition, $M_i$, $m_i$. But instead of $ Delta x_i$ we use $ Delta alpha_i = alpha(x_i) - alpha(x_{i-1})$ . If $ alpha$ is a smooth function: $ alpha(x_{i}+ Delta x_i) - alpha(x_{i}) = alpha(x_{i}) + Delta x_i alpha&#39;(x_{i}) - alpha(x_{i}) = Delta x_i alpha&#39;(x_{i})$. This is the first order approx. So, the derivative of $ alpha$ is what governs the integral. | . So the definition now is similar to Reimann. but instead of $dx$ use $d alpha$. If upper and lower integral match, then $f in R( alpha)$. If $ alpha(x) = x$, it is same as Reimann. . | If f moves around a lot in short interval, then it is difficult for $m_i$ to conervge to same number. For Stieltjes, $ alpha$ too has to be same property i.e. its moving around a lot will make the integral undefined. . | . Holder Continuity . Now we want to define the continuity. It extends to functions which are differentiable as well as not differentiable. A function , $f$, is locally holder continuous with exponent $ alpha in [0,1]$ if, . $$ frac{|f(t+h)-f(t)|}{|h|^{ alpha}} leq C leq infty$$ . For regular derivative, we define $ alpha = 1$. If we allow $ alpha$ to be $&lt; 1$, then as $h rightarrow 0$, the $h^{ alpha}$ will not tend to zero as quickly because $ alpha$ will slow it down. So it is easier for the ratio to be finite. . | The larger $ alpha$, the stronger the condition. . | $ alpha = 0.5$ is for brownian motion. They are not smooth enough to be differentiable. But they are not completely differentiable! . | . Failure of an extension of Riemann-Stieltjes Integral is what leads us to Ito calculus! . Coming back to Stieltjes Integral . If $ alpha$ differentiable, $ int f d alpha = int f alpha&#39; dx$, i.e. just the inegration is scaled with $ alpha&#39;$ | . $$ int alpha d alpha = int alpha alpha&#39; dx = int_a^b frac{1}{2} frac{d}{dx} alpha^2 (x) |_a^b = frac{ alpha^2 (b)}{2} - frac{ alpha^2 (a)}{2} $$ . Young&#39;s theorem . It says that the joint codition of $f$ and $ alpha$ with exponents $a$, $b$ respectively, $a+b&gt;1$ then $f in R( alpha)$ i.e. Stieltjes integral. . Examples: . $$f(x) = e^x, alpha(x) = theta(x- 1/2) theta~~~ text{is heavyside}~~~$$ $d alpha = 0$ if $$ . Answer is $e^{ frac{1}{2}}$ So, this integral is more useful than riemann. This is another reason why the stiltzer equation is important! .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/02/Math-Integration.html",
            "relUrl": "/2021/03/02/Math-Integration.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Title",
            "content": "We have a need to understand the high dimensional data to find the low dimensional representation, to classify data points, and to predict out-of-sample data points. . Some methods about which I have written blogs are . OLS | SVD | PCA | LDA | . Now we will talk about $L_p$ optimization! ($p geq 1$) . $L_2$ is nice because we have the Hilbert Space property! For p &gt;1, all the $L_p$ are norm spaces. They all differ by the distance function we choose. Its like what type of glass we put on. They also signifiy which type of distance to give more weight to. Some glasses are sensitive to a certain type of distances. . In the below figure (from strang&#39;s book), for the 2-norm, the circle is the level set and it is the closest level set to the origin. This point is not same for the L1 norm. For l1, sum of x and y coordinate has to be same. L-1 norm is much less interested in balancing x and y coordinates like L2. it just looks at the sum. So, intuitively it puts more weight on the y-coordinate because its coeeficient is 4. So, this leads to a sparse solution. . For the $L_ infty$ is completely opposite to $L1$. It is completely balancing both the coordinates. So, the solution comes at the location where x=y. . **Why L1 norm?** . Let&#39;s say we have noisy data, and we want to do OLS. There are some extreme outliers. $min_c ||y = cx ||_p$ . Because L2 norm wants to balance out between all the points. (because the square of the outlier error will contribute largely). So the line will be tilted towards the outlier. If we do L_inf then there will be even more correction towards the outlier. . In this case, the L1 norm will not care about the outlier. It will chose to remain closer to most of the points at the cost of being distant from the outlier. . Its a bit tricky to solve L1 problem though in recenet years there are research on this. L1 is a concave optimization problem so it is not that much difficult as well. . L1 is sparse and is useful if you have extremely large dataset (maybe be a billion dimention space). So you would want to put some extra costs to having many many non-zero parameters. L1 will find a sparse structure. . We can take this even further. If we take the $0&lt;p&lt;1$, the optimization is still valid but the $||a-b||_p$ is no longer a distance function. Because the triangle inequality fails! . $$ ||1, 1||_p = 2^{1/p} &gt; 2 = ||1, 0||_p + ||0, 1||_p $$ . This means that the property that we are familiar with i.e. &quot;shortest distance between two points is a straight line&quot; breaks down. . So if you want to go from (0,0) to (1,1) -- we wont travel in a straight line. We will go to (0,1) and then to (1, 1). This is very exotic space. Mathematically we will have to move to topological vector spaces which is more advanced than the so familiar metric spaces. . **Why we are interested?** . The smaller it gets, the sparser the solution is. So there is a very huge penalty for the small numbers. So we can see that in the limiting case the level sets becomes very degenrate i.e. the only thing it cares about is having as less non-zero coefficients. . For L_0 case, it is exactly becomes a minimizing the non-zero components in the estimation. The level sets are the axes. But historically, it has been a NP-Hard problem to solve this optimization. There is no steepest descent you can use to solve this extremely non-concave optimization. . So it turns out, L1 optimization gives avery good approximation for L0 optimization. . Below figure shows how the norms vary with x. For L1, the minima is at x=0. and it keeps on shifting right side with increment of p upto 1/7. For p &lt; 1, it shoots up when x is non-zero. . Basis pursuit is L1, LASSO is joint L2 and L1 where we minimize the joint case. But as both are concave, we would be able to generate efficient solutions. .",
            "url": "https://pradeeptadas.github.io/bquant/2021/03/01/Math-HighDimentionalData.html",
            "relUrl": "/2021/03/01/Math-HighDimentionalData.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Real Analysis",
            "content": "Relation . Subset of product set $R subset A times B$ . Function . Relation on $A times B$, such that A related to B. It can be onto / one-to-one / bijection. . Convergence . We know the cauchy sequence, 1.2, 1.22, 1.222, ... . But Does this converge? To generalize this concept, we have to introduct Metric space. . Metric Space . It has the distance function. with triangle inequality. . It has the concept of Convergence which means $ forall epsilon&gt;0, ~~ exists N: n geq N implies d(x_n, x) &lt; epsilon$ . Similarly, cauchy sequence means $ forall epsilon &gt; 0, ~~ M geq N, implies d(x_n, x) &lt; epsilon$. This is weaker concept than convergence and it has no reference point like convergence. . In complete metric spaces, cauchy sequence converges as well. . For example, $1, 1.41, 1.414, ... rightarrow sqrt{2}$ is a cachy sequency in $ mathbb{Q}$ but converges in $ mathbb{R}$ . Vector Space . Metric space gives abstract space for distance. Similarly, this is abstract space for vector operation. . It has $ times : F times V rightarrow V$ (F is called field here) It also has $+ : V + V rightarrow V$ . Normed Space Vector + Metric Space (with &quot;norm&quot; distance function) . norm funcion $ parallel cdot parallel$: $V rightarrow mathbb{R}_+$ . Norm makes V into special metric space since $d(u, v) = parallel u-v parallel$ satisfies axioms of metric space. . $ boxed{ text{Complete norm space is called Banach Space.}}$ . Inner Product Space Vector Space with inner product . $&lt; cdot, cdot&gt;: V times V rightarrow mathbb{R}$ . This is a special case of normed space $ implies parallel u parallel = sqrt{&lt;u, v&gt;}$ . $ boxed{ text{Complete inner product space is called Hilbert Space.}}$ . Inner product space adds the concept of angles. . $ frac{&lt;u, v&gt;}{ parallel u parallel parallel v parallel} = cos phi$ .",
            "url": "https://pradeeptadas.github.io/bquant/ghtop",
            "relUrl": "/ghtop",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Brain Teasers",
            "content": "2.1 . Screwy Pirates - go from the end. For any 2n+1 pirate case, most senior will offer pirates 1, 3, .. 2n-1 one coin each and keep rest himself. . | Tiger and Sheep If the number of tigers is odd, the sheep will be eaten. Otherwise, not. . | 2.2 . River Crossing A - 10 mins, B - 5mins, C - 2mins, D - 1mins. Min time to cross the river. Key points is that 10 and 5 should go together. C &amp; D go first, 2 mins. A and B go across. C and D cross again. . | Birthday Problem Mar 4, 5, 8. Jun 4, 7. Sep 1, 5. Dec 1, 2, 8. A told you month of birthday and to C his day. After that you said &quot;I don&#39;t know A&#39;s birth day; C doesn&#39;t know it either.&quot; --&gt; This means Dec and Jun are eliminated as they have unique dates. After hearing this C told &quot;I didn&#39;t know A&#39;s birthday but now I know it.&quot; Means that (Mar) 4, (Mar, Sep) 5, (Mar) 8, (Sep) 1. Means the date is unique. So, Mar/Sep 5 is eliminated. You then smiled and told &quot;Now I know it too.&quot; So month is also unique. So its Sep 1. . | Card Game 52 cards. 2 cards at a time. Both black -&gt; dealer&#39;s pile. Both red -&gt; your pile. 1 each -&gt; discarded. If you have more cards in pile then you win 100$. Else you get nothing. Howmuch you will pay to play the game? -- Discarded cards have equal red or black. So,dealer will have same red as you. So, you will never win. . | Burning ropes 2 ropes which take 1hr each to burn. Different densities. How to use to measure 45 mins. Light both ends of first rope to get 30mins. In the sametime light one end of second rope. After first rope is completed, burn the other end of 2nd rope as well to get 15mins. . | Defective ball 12 identical balls. One of the balls is heavier or lighter than the rest. How to determine which is defective with 3 measurements? - Separate the original group into three sets. Comparison of two gives info about third. So, 3^n balls can be done in n steps. If you dont have any info about ball is lighter or lower, then in n measurements, you can identify the ball in (3^n - 3)/2 balls. . | Trailing Zeros Trailing zeros in 100!. Freq of 2 is more than 5. So count 5. 100/5 + 100/25. = 24. . | Horse race 25 horses. 5 can race at a time. 3 fastest horses. Howmany races? 5 races to get faster in each group. Then race amongst them. Then accordingly race another round. So 3 rounds. . | Infinite Sequence x^x^... = 2 $ implies x^2 = 2 implies x = sqrt{2}$ . | 2.3 . 11. .",
            "url": "https://pradeeptadas.github.io/bquant/interview/puzzle/2021/01/30/BookSummary-Greenbook.html",
            "relUrl": "/interview/puzzle/2021/01/30/BookSummary-Greenbook.html",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Important rules",
            "content": "f(x)=xTAx  ⟹  ∂f∂x=(AT+A)xf(x)=x^TAx implies frac{ partial f}{ partial x} = (A^T+A)xf(x)=xTAx⟹∂x∂f​=(AT+A)x . f(β)=2β′x  ⟹  ∂∂βf(β)=2βf( beta) = 2 beta&amp;#x27; x implies frac{ partial}{ partial beta} f( beta) = 2 betaf(β)=2β′x⟹∂β∂​f(β)=2β . Cover letter: https://www.themuse.com/advice/cover-letter-examples-every-type-job-seeker?sc_src=email_891957&amp;sc_lid=59217234&amp;sc_uid=SS5WpSeD5X&amp;sc_llid=201685&amp;sc_eh=574eefa43680980c1&amp;utm_source=emarsys&amp;utm_medium=email&amp;utm_campaign=daily_20210203NoSpon_90Day_891957&amp;utm_content=sponsored&amp;utm_term=&amp;uid=887470193 .",
            "url": "https://pradeeptadas.github.io/bquant/mathematics/2021/01/25/matrix-derivative-formulae.html",
            "relUrl": "/mathematics/2021/01/25/matrix-derivative-formulae.html",
            "date": " • Jan 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a quant by profession. Enthusiatic about computers, mathematics and Formula one! . You can find my resume at: Resume Link .",
          "url": "https://pradeeptadas.github.io/bquant/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pradeeptadas.github.io/bquant/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}